{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243e38f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POLI 175 - Lecture 14\n",
    "\n",
    "## Tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4046a13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tree-based methods II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879690",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7a6812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Packages Install (take some time)\n",
    "using Pkg\n",
    "Pkg.add(\"MLJDecisionTreeInterface\")\n",
    "Pkg.add(\"DecisionTree\")\n",
    "Pkg.add(\"GraphViz\")\n",
    "Pkg.add(\"EvoTrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d345379d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree-based methods\n",
    "\n",
    "Tree-based methods consist of segmenting the predictors' space into many regions, then use these regions to predict the target variable.\n",
    "- We use a heuristic prediction here, such as the variable's mean in the region for *regression*.\n",
    "- Or the most frequent observation in the region for *classification*.\n",
    "    \n",
    "This approach is called the `decision tree method`.\n",
    "\n",
    "By itself it is terrible. But we will discuss many methods that improve efficiency considerably."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79543cb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55c5cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onehot! (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Packages Here\n",
    "using DataFrames\n",
    "using MLJ, MLJIteration\n",
    "import MLJLinearModels, MLJBase, MLJModels\n",
    "import MultivariateStats, MLJMultivariateStatsInterface\n",
    "import CSV, Plots, GLM, StatsBase, Random\n",
    "import LaTeXStrings, StatsPlots, Lowess, Gadfly, RegressionTables\n",
    "import CovarianceMatrices, Econometrics, LinearAlgebra, MixedModelsExtras\n",
    "import Missings, StatsAPI, FreqTables, EvalMetrics\n",
    "import NearestNeighborModels\n",
    "\n",
    "# Decision tree stuff\n",
    "import MLJDecisionTreeInterface, DecisionTree, GraphViz, EvoTrees\n",
    "\n",
    "# Adapted from @xiaodaigh: https://github.com/xiaodaigh/DataConvenience.jl\n",
    "function onehot!(df::AbstractDataFrame, \n",
    "        col, cate = sort(unique(df[!, col])); \n",
    "        outnames = Symbol.(col, :_, cate))\n",
    "    transform!(df, @. col => ByRow(isequal(cate)) .=> outnames)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9e249f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4704653f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>3×11 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">age</th><th style = \"text-align: left;\">statusquo</th><th style = \"text-align: left;\">vote</th><th style = \"text-align: left;\">voteyes</th><th style = \"text-align: left;\">region_M</th><th style = \"text-align: left;\">region_N</th><th style = \"text-align: left;\">region_S</th><th style = \"text-align: left;\">region_SA</th><th style = \"text-align: left;\">education_PS</th><th style = \"text-align: left;\">education_S</th><th style = \"text-align: left;\">sex_F</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"InlineStrings.String1\" style = \"text-align: left;\">String1</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th><th title = \"Bool\" style = \"text-align: left;\">Bool</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">65</td><td style = \"text-align: right;\">1.0082</td><td style = \"text-align: left;\">Y</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">true</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">29</td><td style = \"text-align: right;\">-1.29617</td><td style = \"text-align: left;\">N</td><td style = \"text-align: right;\">0</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">true</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">true</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">38</td><td style = \"text-align: right;\">1.23072</td><td style = \"text-align: left;\">Y</td><td style = \"text-align: right;\">1</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">true</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">false</td><td style = \"text-align: right;\">true</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& age & statusquo & vote & voteyes & region\\_M & region\\_N & region\\_S & region\\_SA & education\\_PS & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & String1 & Int64 & Bool & Bool & Bool & Bool & Bool & \\\\\n",
       "\t\\hline\n",
       "\t1 & 65 & 1.0082 & Y & 1 & 0 & 1 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\t2 & 29 & -1.29617 & N & 0 & 0 & 1 & 0 & 0 & 1 & $\\dots$ \\\\\n",
       "\t3 & 38 & 1.23072 & Y & 1 & 0 & 1 & 0 & 0 & 0 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×11 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m age   \u001b[0m\u001b[1m statusquo \u001b[0m\u001b[1m vote    \u001b[0m\u001b[1m voteyes \u001b[0m\u001b[1m region_M \u001b[0m\u001b[1m region_N \u001b[0m\u001b[1m region_S \u001b[0m\u001b[1m regio\u001b[0m ⋯\n",
       "     │\u001b[90m Int64 \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m String1 \u001b[0m\u001b[90m Int64   \u001b[0m\u001b[90m Bool     \u001b[0m\u001b[90m Bool     \u001b[0m\u001b[90m Bool     \u001b[0m\u001b[90m Bool \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │    65    1.0082   Y              1     false      true     false      f ⋯\n",
       "   2 │    29   -1.29617  N              0     false      true     false      f\n",
       "   3 │    38    1.23072  Y              1     false      true     false      f\n",
       "\u001b[36m                                                               4 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading the data\n",
    "chile = CSV.read(\n",
    "    download(\"https://raw.githubusercontent.com/umbertomig/POLI175julia/main/data/chilesurvey.csv\"), \n",
    "    DataFrame,\n",
    "    missingstring = [\"NA\"]\n",
    "); dropmissing!(chile)\n",
    "chile.voteyes = ifelse.(chile.vote .== \"Y\", 1, 0)\n",
    "\n",
    "# One-hot encoding (we will learn a better way to do it later)\n",
    "onehot!(chile, :region);\n",
    "onehot!(chile, :education);\n",
    "onehot!(chile, :sex);\n",
    "\n",
    "# Drop reference categories\n",
    "select!(chile, Not(:region, :income, :population, :sex, :education, :region_C, :education_P, :sex_M))\n",
    "\n",
    "# Checking\n",
    "first(chile, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f69ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Class Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b696a377",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>3×9 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">education</th><th style = \"text-align: left;\">income</th><th style = \"text-align: left;\">young</th><th style = \"text-align: left;\">urban</th><th style = \"text-align: left;\">states</th><th style = \"text-align: left;\">educ_log</th><th style = \"text-align: left;\">income_log</th><th style = \"text-align: left;\">urban_log</th><th style = \"text-align: left;\">young_log</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th><th title = \"InlineStrings.String3\" style = \"text-align: left;\">String3</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: right;\">189</td><td style = \"text-align: right;\">2824</td><td style = \"text-align: right;\">350.7</td><td style = \"text-align: right;\">508</td><td style = \"text-align: left;\">ME</td><td style = \"text-align: right;\">5.24175</td><td style = \"text-align: right;\">7.94591</td><td style = \"text-align: right;\">6.23048</td><td style = \"text-align: right;\">5.85993</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: right;\">169</td><td style = \"text-align: right;\">3259</td><td style = \"text-align: right;\">345.9</td><td style = \"text-align: right;\">564</td><td style = \"text-align: left;\">NH</td><td style = \"text-align: right;\">5.1299</td><td style = \"text-align: right;\">8.08918</td><td style = \"text-align: right;\">6.33505</td><td style = \"text-align: right;\">5.84615</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: right;\">230</td><td style = \"text-align: right;\">3072</td><td style = \"text-align: right;\">348.5</td><td style = \"text-align: right;\">322</td><td style = \"text-align: left;\">VT</td><td style = \"text-align: right;\">5.43808</td><td style = \"text-align: right;\">8.03008</td><td style = \"text-align: right;\">5.77455</td><td style = \"text-align: right;\">5.85364</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& education & income & young & urban & states & educ\\_log & income\\_log & urban\\_log & young\\_log\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Float64 & Int64 & String3 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 189 & 2824 & 350.7 & 508 & ME & 5.24175 & 7.94591 & 6.23048 & 5.85993 \\\\\n",
       "\t2 & 169 & 3259 & 345.9 & 564 & NH & 5.1299 & 8.08918 & 6.33505 & 5.84615 \\\\\n",
       "\t3 & 230 & 3072 & 348.5 & 322 & VT & 5.43808 & 8.03008 & 5.77455 & 5.85364 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×9 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m education \u001b[0m\u001b[1m income \u001b[0m\u001b[1m young   \u001b[0m\u001b[1m urban \u001b[0m\u001b[1m states  \u001b[0m\u001b[1m educ_log \u001b[0m\u001b[1m income_log \u001b[0m\u001b[1m urban\u001b[0m ⋯\n",
       "     │\u001b[90m Int64     \u001b[0m\u001b[90m Int64  \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m String3 \u001b[0m\u001b[90m Float64  \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │       189    2824    350.7    508  ME        5.24175     7.94591    6.2 ⋯\n",
       "   2 │       169    3259    345.9    564  NH        5.1299      8.08918    6.3\n",
       "   3 │       230    3072    348.5    322  VT        5.43808     8.03008    5.7\n",
       "\u001b[36m                                                               2 columns omitted\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Education Expenditure Dataset\n",
    "educ = CSV.read(download(\"https://raw.githubusercontent.com/umbertomig/POLI175julia/main/data/educexp.csv\"), DataFrame)\n",
    "\n",
    "# Processing\n",
    "educ.educ_log = log.(educ.education);\n",
    "educ.income_log = log.(educ.income)\n",
    "educ.urban_log = log.(educ.urban)\n",
    "educ.young_log = log.(educ.young)\n",
    "\n",
    "# Checking\n",
    "first(educ, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731da624",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac3b0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(\n",
    "    educ[:, [\"education\", \"income\", \"young\", \"urban\"]], \n",
    "    ==(:education);              ## Target (all else features...)\n",
    "    :education => Continuous,    ## Var types\n",
    "    :income    => Continuous,\n",
    "    :young     => Continuous,\n",
    "    :urban     => Continuous\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb498fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ\n",
    "\n",
    "Train-test split like a boss: Use the indexes to do the split.\n",
    "\n",
    "```julia\n",
    "train, test = partition(\n",
    "    eachindex(y),   ## Index with the eachindex(.) method\n",
    "    0.7,            ## Proportion in the training set\n",
    "    shuffle = true, ## Shuffle the data,\n",
    "    rng = 12345     ## Random seed (ensure same results; not necessary)\n",
    ") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb655838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(\n",
    "    eachindex(y),   ## Index with the eachindex(.) method\n",
    "    0.7,            ## Proportion in the training set\n",
    "    shuffle = true, ## Shuffle the data,\n",
    "    rng = 12345     ## Random seed (ensure same results; not necessary)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06edb41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc717b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTraining machine(DecisionTreeRegressor(max_depth = 5, …), …).\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the regressor\n",
    "dtreg = MLJDecisionTreeInterface.DecisionTreeRegressor(max_depth = 5);\n",
    "\n",
    "# Create the machine\n",
    "mach = machine(dtreg, X, y);\n",
    "\n",
    "# Fit the machine for the training set (note the rows parameter)\n",
    "fit!(mach, rows = train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb7f0e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd87e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1191.2627962962965\n",
      "\n",
      "Root-Mean Squared Error (residual SE): 34.51467508606008\n"
     ]
    }
   ],
   "source": [
    "# Compute the predicted values\n",
    "yhat = predict(mach, X[test, :]);\n",
    "\n",
    "# Compute the MLJ default root mean squared error\n",
    "rmse = rms(y[test], yhat);\n",
    "\n",
    "# Print (note the $(var ^ 2)...it prints the square of the variable.)\n",
    "println(\"Mean Squared Error: $(rmse ^ 2)\")\n",
    "\n",
    "# Print (note the $var...it prints the variable.)\n",
    "println(\"\\nRoot-Mean Squared Error (residual SE): $rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9d529f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ\n",
    "\n",
    "Let us check our decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc82f429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1 < 2911.0 ?\n",
      "├─ Feature 1 < 2606.0 ?\n",
      "    ├─ 132.4 : 0/5\n",
      "    └─ 161.75 : 0/8\n",
      "└─ Feature 1 < 3742.0 ?\n",
      "    ├─ Feature 3 < 739.5 ?\n",
      "        ├─ 213.0 : 0/9\n",
      "        └─ 188.16666666666666 : 0/6\n",
      "    └─ 245.0 : 0/8\n"
     ]
    }
   ],
   "source": [
    "tree_model = fitted_params(mach).raw_tree\n",
    "DecisionTree.print_tree(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac4a756",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Regression in MLJ\n",
    "\n",
    "**Your Turn**: Fit a Decision Tree Classifier using MLJ to predict the vote for pinochet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d87975bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e78812",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c28792",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Ensemble Learning\n",
    "\n",
    "### Weak Learners\n",
    "\n",
    "**Definition**: A model that is only slightly better than random guessing.\n",
    "- Usually a very simple model (e.g., a classification tree!).\n",
    "- Very low accuracy.\n",
    "\n",
    "Why would we want to ever use something like that for prediction?\n",
    "- *Computational efficiency*: Imagine how easy it may be to fit a one-leaf classification tree.\n",
    "- *Lower chance of overfitting*: They will perform poorly, and this guarantees low chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc3727",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Ensemble Learning\n",
    "\n",
    "### Weak Learners\n",
    "\n",
    "Combining multiple weak learners can lead to a powerful prediction, as opposed to the individual weak learners which are not very useful.\n",
    "\n",
    "The keywork here is **combine**. How does that work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c05ad37",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction to Ensemble Learning\n",
    "\n",
    "We do that by using **ensembles**.\n",
    "\n",
    "**Definition**: Ensemble Learning is a technique where multiple \"weak learners\" are trained and combined to solve a specific problem.\n",
    "\n",
    "The goal is to improve the overall accuracy, robustness, and performance of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591f613",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Bootstrap\n",
    "\n",
    "- To understand some ensemble techniques, we must first learn what a bootstrap is.\n",
    "\n",
    "- [**Bootstrap**](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)): Technique to fit models empirically, without deriving theoretically the parameters of interest.\n",
    "    + We use it a lot to find standard errors and run things like [*exact tests*](https://en.wikipedia.org/wiki/Exact_test) and [randomization inference](https://dimewiki.worldbank.org/Randomization_Inference).\n",
    "\n",
    "- Very empirical!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39388f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detour: Bootstrap\n",
    "\n",
    "**Algorithm:** Start with the number of repetitions, N. For each $1, 2, \\cdots, N$ step:\n",
    "\n",
    "1. Draw a sample of the dataset [**with replacement**](https://en.wikipedia.org/wiki/Resampling_(statistics)) that has the same size of the dataset.\n",
    "\n",
    "2. Fit the model (e.g., a regression) in the randomly drawn dataset.\n",
    "\n",
    "3. Save the coefficient of interest.\n",
    "\n",
    "In the end, take the mean of the coefficient as the `bootstrapped` coefficient and the standard deviation as the standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf8fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- Bagging stands for Bootstrap Aggregation.\n",
    "\n",
    "- The idea is to fit each tree on a bootstrapped dataset, then take the average of all trees.\n",
    "\n",
    "- Each tree performs poorly. However, the average performance of all of them is better!\n",
    "\n",
    "$$ \\hat{f}_{bag}(x) \\ = \\ \\dfrac{1}{B}\\sum_{b = 1}^B \\hat{f}^b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed46502",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- We let trees grow wildly: no pruning!\n",
    "    + Averaging them out is what reduces the variance!\n",
    "    \n",
    "- For continuous variables, we take averages as the predicted value.\n",
    "\n",
    "- How about classification problems?\n",
    "    + Majority vote for all trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60396a32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "![bag1](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/bag1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6033e7aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- This straightforward technique decreases the variance of a tree significantly.\n",
    "\n",
    "- But how to interpret what the average of the trees means?\n",
    "    - Well, we lose in terms of interpretation...\n",
    "\n",
    "- One positive thing is that we can still find the **importance of each variable for the *bagging***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101c58bb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "![bag2](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/bag2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716eec95",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging\n",
    "\n",
    "- Cross-validation here can be improved by using something called **out-of-bag** errors:\n",
    "    + The bootstrap process usually leaves out 1/3 of the sample.\n",
    "    + We can take advantage of this left-out (or *out-of-bag* sample), to estimate our models.\n",
    "\n",
    "- And we fit RMSE for continuous or accuracy for discrete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a2ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- It is **not** a place where data scientists go camping.\n",
    "\n",
    "- Random forests intend to improve the effectiveness of our bagging estimates.\n",
    "\n",
    "- Each bagging tree in the ensemble can be highly correlated with each other.\n",
    "    - This messes up the prediction because it reduces the contribution of each tree.\n",
    "\n",
    "- To fix that, we tweak the bagging to *decorrelate* the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76b1d2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- A simple way to do that is only to consider a subset of the predictors at each tree.\n",
    "    - Why would we even want to do that?\n",
    "    \n",
    "- Let a strong predictor with a bunch of other weak ones. Then:\n",
    "    1. All bagging trees will rely on the stronger predictor more than the others.\n",
    "    2. Subsetting the number of variables, considering subsets where the strong predictor is not there, improves the usage of the weak predictors.\n",
    "        - This *decorrelates* the trees!\n",
    "\n",
    "- Rule of thumb: Use $m = \\sqrt{p}$ predictors at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e2f0c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "- Choose a small(er) $m$ if the predictors are all highly correlated.\n",
    "\n",
    "![img](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/rf1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242428cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "- Ensemble method that combines weak learners to form a stronger one.\n",
    "    + Example: Regression tree that is only allowed to have one leaf!\n",
    "\n",
    "- It builds on accumulation: Every predictor tries to improve the predecessor's job.\n",
    "    - Work with the errors of the previous models, and update the fit slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a4ebe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "**Algorithm:** Start with a null model ($\\hat{f}(x) = 0$), the residual equals to $r_i = y_i$, and a number $B$ of steps.\n",
    "\n",
    "For each $b \\in \\{1, 2, \\cdots, B\\}$:\n",
    "\n",
    "1. Fit a tree $\\hat{f}^b(x)$ with $d$ splits (or d+1 terminal nodes).\n",
    "\n",
    "2. Set:\n",
    "\n",
    "$$ \\hat{f}_{new}(x) = \\hat{f}_{old}(x) + \\lambda \\hat{f}^b(x) $$\n",
    "\n",
    "3. Set \n",
    "\n",
    "$$ r_{i_{new}} = r_{i_{old}} - \\lambda \\hat{f}^b(x) $$\n",
    "\n",
    "At the end, you should define $\\hat{f}(x)$ as:\n",
    "\n",
    "$$ \\hat{f}(x) = \\sum_{b=1}^B \\lambda \\hat{f}^b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56912cd6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "- You can overfit using boosting. But only if $B$ is too large.\n",
    "\n",
    "- $\\lambda$: Controls the rate that your boosting algorithm is learning.\n",
    "    + Small $\\lambda$s require large $B$s\n",
    "\n",
    "- $d$: Controls the complexity of each step. $d=1$ tends to work well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655eaa21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "![imgb](https://upload.wikimedia.org/wikipedia/commons/b/b5/Ensemble_Boosting.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de5352b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Lecture\n",
    "\n",
    "Next lecture we will fit:\n",
    "\n",
    "- Bagging\n",
    "- Random Forests\n",
    "- Boosting\n",
    "\n",
    "Using MLJ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eace45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d252e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you next class\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
