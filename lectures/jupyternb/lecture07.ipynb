{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "243e38f9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POLI 175 - Lecture 07\n",
    "\n",
    "## Regression (final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15c870",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression and Classification\n",
    "\n",
    "Loading packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b94fa",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# If needed\n",
    "using Pkg\n",
    "Pkg.add(\"Lowess\")\n",
    "Pkg.add(\"Gadfly\")\n",
    "Pkg.add(\"RegressionTables\")\n",
    "Pkg.add(\"CovarianceMatrices\")\n",
    "Pkg.add(\"Econometrics\")\n",
    "Pkg.add(\"LinearAlgebra\")\n",
    "Pkg.add(\"MixedModelsExtras\")\n",
    "Pkg.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2e2d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression and Classification\n",
    "\n",
    "Loading packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ca894",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Loading the data\n",
    "using CSV, DataFrames, Plots, GLM, StatsBase, Random\n",
    "using LaTeXStrings, StatsPlots, Lowess, Gadfly, RegressionTables\n",
    "using CovarianceMatrices, Econometrics, LinearAlgebra, MixedModelsExtras\n",
    "\n",
    "# Auxiliar function\n",
    "function pairplot(df)\n",
    "    _, cols = size(df)\n",
    "    plots = []\n",
    "    for row = 1:cols, col = 1:cols\n",
    "        push!(\n",
    "            plots,\n",
    "            scatter(\n",
    "                df[:, row],\n",
    "                df[:, col],\n",
    "                xtickfont = font(4),\n",
    "                ytickfont = font(4),\n",
    "                legend = false,\n",
    "            ),\n",
    "        )\n",
    "    end\n",
    "    Plots.plot(plots..., layout = (cols, cols))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cdd28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression and Classification\n",
    "\n",
    "Loading data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416830bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the prestige dataset\n",
    "urldat = \"https://raw.githubusercontent.com/umbertomig/POLI175julia/main/data/Duncan.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "dat = CSV.read(download(urldat), DataFrame)\n",
    "\n",
    "# First few obs\n",
    "first(dat, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a185f0fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "There are many packages in Julia to run Regression and Classification models. We are going to use two:\n",
    "\n",
    "- [`GLM`](https://github.com/JuliaStats/GLM.jl) and its family (https://juliastats.org/)\n",
    "- [`MLJ`](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "\n",
    "We have been using the [GLM](https://github.com/JuliaStats/GLM.jl). Next lecture we will center on the [MLJ](https://alan-turing-institute.github.io/MLJ.jl/dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71cff81",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "### Questions\n",
    "\n",
    "Quick reminder of a few relevant questions on the prestige of professions dataset:\n",
    "\n",
    "Questions we answered:\n",
    "\n",
    "- Is there a relationship between `prestige` and `income`?\n",
    "- How strong is the relationship between `prestige` and `income`?\n",
    "- Which variables are associated with `prestige`?\n",
    "- How can we accurately predict the prestige of professions not studied in this survey?\n",
    "\n",
    "Questions that we are still to answer:\n",
    "\n",
    "- Is the relationship linear?\n",
    "- Is there a synergy among predictors?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7837fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regression\n",
    "\n",
    "### Simplest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4788cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = lm(@formula(prestige ~ income), dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3f024b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Several plots can help us diagnose the quality of our model.\n",
    "\n",
    "**Warning**: Find and analyzing these violations is **more of an art**.\n",
    "\n",
    "A careful analysis is frequent enough to ensure you have a `good` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52365b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "When the relationship is non-linear, you could have done better using a different (more flexible) functional form.\n",
    "\n",
    "The plot to detect this is residual in the y-axis against the fitted values in the x-axis:\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/fig5.png?raw=true)\n",
    "\n",
    "**Plot**: Fitted Values x Raw Residuals\n",
    "\n",
    "- Good: You should find no patterns.\n",
    "\n",
    "- Bad: A discernible pattern tells you that you could have done better with a more flexible model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e80dbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "But how to find the fitted values and the residuals in the GLM package?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals\n",
    "p1 = histogram(residuals(mod1))\n",
    "title!(\"Residuals\")\n",
    "p2 = histogram(fitted(mod1))\n",
    "title!(\"Fitted Values\")\n",
    "Plots.plot(p1, p2, layout = (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55de51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "For the `prestige` x `income` relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5460b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Plots.scatter(\n",
    "    fitted(mod1), residuals(mod1),\n",
    "    xlabel = \"Fitted\", ylabel = \"Rediduals\",\n",
    "    series_annotations = text.(dat.profession, :left, :bottom, 8),\n",
    "    legend = false\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f04a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "Hint: Look at the smoothing trend line (the `lowess`). You should see no discernible trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f11cd89",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Gadfly.plot(x=fitted(mod1), y=residuals(mod1), \n",
    "    Geom.point, Geom.smooth, Guide.xlabel(\"Fitted\"), \n",
    "    Guide.ylabel(\"Residuals\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90276c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "Let's `cook` a non-linear relation:\n",
    "\n",
    "$$ Y = 2 + X + 2 X^2 + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc76de4",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Cooking\n",
    "Random.seed!(4321)\n",
    "cooked_data = DataFrame(x = randn(100))\n",
    "cooked_data.y = 2 .+ 1 .* cooked_data.x .+ 2 .* (cooked_data.x .^ 2) .+ randn(100)\n",
    "\n",
    "## Fitting (wrong)\n",
    "mod2 = lm(@formula(y ~ x), cooked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f21ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "The smoothing trend line (the `lowess`) show a discernible trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5edcd04",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Gadfly.plot(x=fitted(mod2), y=residuals(mod2), \n",
    "    Geom.point, Geom.smooth, Guide.xlabel(\"Fitted\"), \n",
    "    Guide.ylabel(\"Residuals\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156b0d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "Let us fit the *right* model now: $$ Y = 2 + X + 2 X^2 + \\varepsilon $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = lm(@formula(y ~ x + exp(x)), cooked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bd6d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "And the residuals versus fitted values look better when fitting the correctly specified model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef40596",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Gadfly.plot(x=fitted(mod3), y=residuals(mod3), \n",
    "    Geom.point, Geom.smooth, Guide.xlabel(\"Fitted\"), \n",
    "    Guide.ylabel(\"Residuals\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe61f17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Non-linearity\n",
    "\n",
    "There exist a test called [Ramsey RESET test](https://en.wikipedia.org/wiki/Ramsey_RESET_test).\n",
    "\n",
    "I strongly suggest you not to use these, since it usually does not identify better relationships than polynomial, while non-linearity can be something extremely complex.\n",
    "\n",
    "We will learn how to deal with this in a few lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adceec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"cite2c-biblio\"></div>## Diagnostics\n",
    "\n",
    "### Heteroscedasticity\n",
    "\n",
    "It is fancy wording to say that the variance in error is not constant.\n",
    "\n",
    "It usually means that you are better at fitting some range of the predictors than others.\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/fig7.png?raw=true)\n",
    "\n",
    "**Plot:** Fitted Values x Residuals\n",
    "\n",
    "- Bad (left-hand side): A funnel-shaped figure tells you that you may have heteroscedasticity. It invalidates simple standard errors assumptions.\n",
    "\n",
    "- Good (right-hand side): You should find no patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eae725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Heteroscedasticity\n",
    "\n",
    "For the `prestige` x `income` relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7779b1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Gadfly.plot(\n",
    "    x = fitted(mod1), y = residuals(mod1),\n",
    "    label = dat.profession,\n",
    "    Geom.point, \n",
    "    Geom.smooth,\n",
    "    Geom.label,\n",
    "    Guide.xlabel(\"Fitted\"), \n",
    "    Guide.ylabel(\"Residuals\"),\n",
    "    Guide.title(\"Fitted versus raw residual plot\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d1a6b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Heteroskedasticity\n",
    "\n",
    "Let us `cook` a heteroskedastic model:\n",
    "\n",
    "$$ Y = 2 + 3 X + \\tilde{\\varepsilon} $$\n",
    "\n",
    "Where $\\text{Cov}[\\tilde{\\varepsilon}] \\ \\neq \\ \\sigma^2I$.\n",
    "\n",
    "In this particular case, let us make the variance of the residuals to look like a football:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16600c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cooking (Het-error term)\n",
    "Random.seed!(4321)\n",
    "cooked_data = DataFrame(x = randn(100))\n",
    "cooked_data.y = 2 .+ 3 .* cooked_data.x .+ (maximum(cooked_data.x .^ 2 .+ 1) .- ((cooked_data.x).^2)) .* randn(100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70720f2d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Fitting\n",
    "mod4 = lm(@formula(y ~ x), cooked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fccda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Heteroscedasticity\n",
    "\n",
    "For the cooked data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa5536",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Residual x fitted values (linearity + heteroscedasticity)\n",
    "Gadfly.plot(x=fitted(mod4), y=residuals(mod4), \n",
    "    Geom.point, Geom.smooth, Guide.xlabel(\"Fitted\"), \n",
    "    Guide.ylabel(\"Residuals\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396110b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Heteroscedasticity\n",
    "\n",
    "Our standard errors are wrong. So are our:\n",
    "\n",
    "1. Confidence Intervals\n",
    "1. P-values\n",
    "1. T-stats\n",
    "\n",
    "In these cases, we need **robust standard errors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Errors for the heteroskedastic model\n",
    "CovarianceMatrices.stderror(mod4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d793e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected Standard Errors for the heteroskedastic model\n",
    "# Note: several types of corrections...\n",
    "CovarianceMatrices.stderror(CovarianceMatrices.HC1(), mod4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555c23b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Outliers\n",
    "\n",
    "- Outliers are values very far away from most values predicted by the model.\n",
    "\n",
    "- Sometimes, it is correct, but frequently it may tell you that you made a mistake in collecting the data!\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/fig8.png?raw=true)\n",
    "\n",
    "**Plot**: Fitted x Studentized residuals (right-hand side plot)\n",
    "\n",
    "- Best: You should find no extreme values in the plot.\n",
    "\n",
    "- Bad: An extreme value can affect your RSE, $R^2$, and mess up with p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e61db4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Outliers\n",
    "\n",
    "A few important measurements (all vectors, computed for each data point):\n",
    "\n",
    "[**Studentized residual**](https://en.wikipedia.org/wiki/Studentized_residual): Residual weighted by the leverage of the point. Studentized because it follows the [Student's t distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution).\n",
    "\n",
    "[**Leverage**](https://en.wikipedia.org/wiki/Leverage_(statistics)): It represents how far away a given *target* value is compared to other *target* values $\\bigg(\\dfrac{\\partial \\hat{y}_i}{\\partial y_i}\\bigg)$.\n",
    "\n",
    "[**Cook's Distance**](https://en.wikipedia.org/wiki/Cook%27s_distance): It measures how much the regression model changes when we remove a given observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf37c5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#= We need to compute the studentized residual\n",
    "   (https://en.wikipedia.org/wiki/Studentized_residual)\n",
    "   I wrote this function to facilitate the work\n",
    "=#\n",
    "function lm_measures(lmod)\n",
    "    X = modelmatrix(lmod)\n",
    "    RSS = sum((residuals(lmod)).^2)\n",
    "    sigma_hat = sqrt(RSS/dof_residual(lmod))\n",
    "    leverage = diag(X * inv(transpose(X) * X) * transpose(X))\n",
    "    studentized_resid = residuals(lmod) ./ (sigma_hat .* sqrt.(1 .- leverage))\n",
    "    return leverage, studentized_resid, cooksdistance(lmod)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6048e42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea1f343",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Gadfly.plot(\n",
    "    x=fitted(mod1), y=lm_measures(mod1)[2], \n",
    "    Geom.point, label = dat.profession, \n",
    "    Guide.xlabel(\"Fitted\"),\n",
    "    Guide.ylabel(\"Studentized Residuals\"), \n",
    "    Geom.label,\n",
    "    yintercept = [-2.0, 2.0], \n",
    "    Geom.hline()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd659e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cc498",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "show(dat, allrows = true, allcols = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd56c870",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db480ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee32f49",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mod2 = lm(@formula(prestige ~ income), \n",
    "    dat[(dat.profession .!= \"minister\"), :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8fff1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod3 = lm(@formula(prestige ~ income), \n",
    "    dat[(dat.profession .!= \"conductor\"), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08069c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod4 = lm(@formula(prestige ~ income), \n",
    "    dat[(dat.profession .!= \"minister\") .& (dat.profession .!= \"conductor\"), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e5cdd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### High Leverage\n",
    "\n",
    "Having a very unusual value, that could potentially tilt the regression line towards it\n",
    "\n",
    "***If high leverage and outlier, bad combination!***\n",
    "\n",
    "![reg](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/fig9.png?raw=true)\n",
    "\n",
    "**Plot**: Leverage x Studentizided residuals\n",
    "\n",
    "- Best: You should find no extreme values in the plot.\n",
    "\n",
    "- Bad: An extreme value can affect your fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1372a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "Gadfly.plot(\n",
    "    x=lm_measures(mod1)[1], y=lm_measures(mod1)[2], \n",
    "    Geom.point, label = dat.profession, \n",
    "    Guide.xlabel(\"Leverage\"),\n",
    "    Guide.ylabel(\"Studentized Residuals\"), \n",
    "    Geom.label,\n",
    "    yintercept = [-2.0, 2.0], \n",
    "    Geom.hline()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aacee14",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Cook's Distance\n",
    "\n",
    "Measures whether removing a given observation tilts the regression coefficients.\n",
    "\n",
    "**Plot**: Cook's Distance\n",
    "\n",
    "- Best: You should find no values in the farther diagonal\n",
    "\n",
    "- Bad: Extreme value in the farther diagonal represents data that is highly influential (high leverage) and high changes in coefficients (high Cook's D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e861597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gadfly.plot(\n",
    "    y = dat.profession, x = lm_measures(mod1)[3], \n",
    "    Geom.point\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba4161",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "When we run multiple linear regression, we add *multicollinearity* to the diagnostics we have seen so far.\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "Multicollinearity is when your predictors are highly correlated. In extreme cases, it messes up with the standard errors in our model (problems with inverse matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c75299a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Pairplot to check\n",
    "println(first(dat, 1))\n",
    "pairplot(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1830c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "### Multicollinearity\n",
    "\n",
    "One measure of multicollinearity is the [*Variance Inflation Factor*](https://en.wikipedia.org/wiki/Variance_inflation_factor).\n",
    "\n",
    "How much the multicollinearity is messing up with the estimates.\n",
    "    \n",
    "To compute, it is fairly easy. As a rule-of-thumb, we would like to see values lower than 5.\n",
    "\n",
    "***It is rarely a problem, though... Especially with large datasets.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63101a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modfull = lm(@formula(prestige ~ income + education + type), dat)\n",
    "println(modfull)\n",
    "MixedModelsExtras.vif(modfull)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42279809",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnostics\n",
    "\n",
    "Rules to diagnostics:\n",
    "\n",
    "1. Always assume heteroskedasticity (use robust standard errors)\n",
    "1. Check for outliers\n",
    "1. Drop a few observations and rerun the regression. Do that with most (all) observations.\n",
    "1. Graph your residuals.\n",
    "1. If you have extreme values, make sure your results remain valid after dropping them.\n",
    "\n",
    "In a nutshell, pay attention to what you are doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b027f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3773c9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "- We usually have a large set of predictors that could be used.\n",
    "    + Which predictors to pick becomes a task.\n",
    "\n",
    "- If we are trying to interpret things and learn from the data, then which predictors are correlated with the outcome is informative:\n",
    "    + Again, picking predictors becomes a task.\n",
    "    \n",
    "- In this and the following lecture, we will learn how to do that systematically. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b72f2f6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "- In here, we are going to consider techniques to select a subset of predictors based on a performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266e9a46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Best Subset Selection\n",
    "\n",
    "**Algorithm:**\n",
    "\n",
    "1. Let $M_0$ denote the null model, which contains no predictors. This model predicts the sample mean for each observation.\n",
    "\n",
    "2. For $k = \\{1, 2, \\cdots, p\\}$:\n",
    "    1. Fit all $p \\choose k$ models with exactly $k$ predictors.\n",
    "    2. Pick the *best* among these models and call it your $M_k$\n",
    "\n",
    "3. Select a single best model from among $M_0, \\cdots, M_p$ using cross-validated prediction error, $C_p$, AIC, BIC, or adjusted $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33edcd3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Best Subset Selection\n",
    "\n",
    "![img ms1](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/ms1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43173a8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "### Subset Selection\n",
    "\n",
    "#### Best Subset Selection\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. You can do this with Logistic Regression: change RSS with [*deviance*](https://en.wikipedia.org/wiki/Deviance_(statistics)).\n",
    "    + In this case, our friend $- 2\\ln({\\hat {L}})$ does very well!\n",
    "\n",
    "2. Best Selection is excellent but fits around $2^p$ models.\n",
    "    + $p = 10$ means around 1000 estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ae8ab3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Forward Selection\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "1. Let $M_0$ denote the null model, which contains no predictors.\n",
    "\n",
    "2. For $k = \\{0, 1, 2, \\cdots, p-1\\}$:\n",
    "    1. Consider all $p - k$ models that augments $M_k$ by one predictor.\n",
    "    2. Pick the *best* among these $p-k$ models, and call it your $M_{k+1}$.\n",
    "\n",
    "3. Select a single best model from among $M_0, \\cdots, M_p$ using cross-validated prediction error, $C_p$, AIC, BIC, or adjusted $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd4cbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Forward Selection\n",
    "\n",
    "- Much more efficient:\n",
    "    + It fits a total of $1 + \\dfrac{p(p+1)}{2}$ models.\n",
    "    + If $p = 20$, the Best Selection would fit 1,048,576\n",
    "    + If $p = 20$, the Forward Step Selection would fit 211 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87217f23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Forward Selection\n",
    "\n",
    "- The catch: It is not guaranteed that it is going to find the *best subset* model.\n",
    "\n",
    "- Example: Let $p = 3$.\n",
    "    + Suppose that the best model involves $v2$ and $v3$.\n",
    "    + But suppose that within the models with only one variable, $v1$ would do better.\n",
    "    + Then, Forward Step Selection would never pick this model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b281b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Forward Selection\n",
    "\n",
    "![img ms2](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/ms2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05794a41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Backward Selection\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "1. Let $M_p$ denote the *full* model, which contains all $p$ predictors.\n",
    "\n",
    "2. For $k = \\{p, p-1, p-2, \\cdots, 1\\}$:\n",
    "    1. Consider all $k$ models that contail all but one predictor in $M_k$, for a total of $k-1$ predictors.\n",
    "    2. Pick the *best* among these $k$ models, and call it your $M_{k-1}$.\n",
    "\n",
    "3. Select a single best model from among $M_0, \\cdots, M_p$ using cross-validated prediction error, $C_p$, AIC, BIC, or adjusted $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697d9d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Backward Selection\n",
    "\n",
    "- Computational Efficiency:\n",
    "    + It fits a total of $1 + \\dfrac{p(p+1)}{2}$ models.\n",
    "    + Same efficiency as the Forward Selection.\n",
    "\n",
    "- Catch:\n",
    "    + Same catch as the Forward Selection: It does not guarantee the pick of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec711f60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### Stepwise Selection: Hybrid Approaches\n",
    "\n",
    "- Combinations of *Forward* and *Backwards* that intend to mimic the *Best Selection*.\n",
    "\n",
    "- Many available.\n",
    "\n",
    "- But the trade-offs are clear: \n",
    "    + Computational efficiency\n",
    "    + Likelihood of picking the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd57b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- For each of the $M_k$ models, this is it:\n",
    "    + RSS: Residual Sum of Squares: We want it to be the lowest possible.\n",
    "    + $R^2$: We want it to be the highest possible.\n",
    "    + And for Logistic or other GLM Regressions, *deviance*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5f488f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- Catches: \n",
    "    1. RSS and $R^2$ always improve with more variables.\n",
    "    2. We want to look at the *testing set goodness-of-fit*, not the *training sets goodness-of-fit*!\n",
    "\n",
    "- And that is why RSS and $R^2$ are not used in Step 3:\n",
    "    - We need something that eventually gets worse the more variables we throw in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9c8d08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "**Important:**\n",
    "\n",
    "- Training Set MSE generally underestimates the testing set MSE.\n",
    "\n",
    "$$ \\text{MSE} \\ = \\ \\dfrac{RSS}{n} $$\n",
    "\n",
    "- But before, we could not split data into *training* and *testing*. \n",
    "    + This is a more recent feature, thanks to our increased computational power.\n",
    "\n",
    "- Here are a few stats that we can fit in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405961b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- [$C_P$](https://en.wikipedia.org/wiki/Mallows%27s_Cp) in a model containing:\n",
    "    - $d$ predictors.\n",
    "    - $n$ observations.\n",
    "    - $\\widehat{\\sigma}^2$ the variance of the error in the full model with all predictors.\n",
    "\n",
    "$$ C_p \\ = \\ \\dfrac{1}{n}(RSS + 2\\times d \\times \\widehat{\\sigma}^2) $$\n",
    "\n",
    "- The smaller, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e341490e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) or $C_p$: AIC is a goodness-of-fit parameters that goes lower when you are improving the model\n",
    "    + But for every variable you add, it penalizes it.\n",
    "    + If by adding more variables, it goes up, then your model is getting more complex without adding much.\n",
    "    \n",
    "$$ \\text{AIC} \\ = \\ 2d - 2\\log({\\hat {L}}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0aa375",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- The maximum likelihood and least squares are the same for models with Gaussian errors.\n",
    "\n",
    "$$ \\text{AIC} \\ = \\ \\dfrac{1}{n} (RSS + 2\\times d \\times \\widehat{\\sigma}^2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96828005",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is another goodness-of-fit, but this one penalizes the addition of variables at a higher rate.\n",
    "\n",
    "$$ \\text{BIC} = k\\log(n) - 2\\log({\\widehat {L}}) $$\n",
    "\n",
    "- Note the difference from the AIC: instead of multiplying by 2, it is multiplying by $\\ln(n)$!\n",
    "\n",
    "- Again, lower values are better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac0e50",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- Or if you want the one with the least square errors:\n",
    "\n",
    "$$ \\text{AIC} \\ = \\ \\dfrac{1}{n} (RSS + \\log(n) \\times d \\times \\widehat{\\sigma}^2)  $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a7dace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- [Adjusted $R^2$](https://en.wikipedia.org/wiki/Bayesian_information_criterion): It is a change in the $R^2$ to penalize the addition of regressors.\n",
    "\n",
    "$$ \\overline{R}^{2} = 1-(1-R^{2})\\dfrac{n-1}{n-d} \\ = \\ 1 - \\dfrac{\\frac{RSS}{n - d - 1}}{\\frac{TSS}{n - 1}}$$\n",
    "\n",
    "- $R^2$ always increase but the $\\overline{R}^2$ may increase or decrese.\n",
    "\n",
    "- **Not like the others:** This one, the higher, the better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e5bb2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "![img ms3](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/ms3.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291cc9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is Best?\n",
    "\n",
    "- $C_P$, AIC, and BIC all have a solid theoretical justification.\n",
    "    + Many of them have something we call Large Sample Properties.\n",
    "    + Check their Wikipedia of them. They converge to nice, important values.\n",
    "\n",
    "- $\\overline{R}^{2}$ does not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceca471",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is best? Validation and Cross-Validation\n",
    "\n",
    "- The main advantage is obvious: You look at testing errors!\n",
    "\n",
    "- The other main advantage is regarding estimating parameters:\n",
    "    + $C_P$, AIC, and BIC all have a strong theoretical justification, which is reassuring.\n",
    "    + But sometimes, one does not know the theory behind an estimate to compute statistics or even standard errors.\n",
    "    + E.g., which $\\widehat{\\sigma}^2$ should we pick?\n",
    "    + Validation and Cross-Validation do great in these cases!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d1ae6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Model Selection\n",
    "\n",
    "### Subset Selection\n",
    "\n",
    "#### What is best? Validation and Cross-Validation\n",
    "\n",
    "![img ms4](https://github.com/umbertomig/POLI175julia/blob/c9b0555e3e97778495bee72746aee43ddf3226d7/img/ms4.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eace45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d252e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# See you next class\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
